{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28fc302-db8e-4db8-ade4-ba34d6fe2d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acc541c-d180-4f49-8bdc-a3d993a1faf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+-----+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|           Revenue|Month|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+-----+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|15.299999999999999|   12|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|             20.34|   12|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|              22.0|   12|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|             20.34|   12|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|             20.34|   12|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Ruta donde se ha subido el archivo en DBFS (ajusta según la ubicación exacta)\n",
    "file_path = \"/FileStore/tables/ecommerce_data_cleaned.csv\"\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar algunas filas para verificar que se cargó correctamente\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da766bc4-330e-4c08-9b3f-72ba6734ac87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertir la columna Month a tipo entero explícitamente\n",
    "df_ml = df.withColumn(\"Month\", col(\"Month\").cast(\"int\"))\n",
    "\n",
    "# Definir las características y la etiqueta\n",
    "df_ml = df_ml.withColumn(\"label\", df[\"Revenue\"])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Quantity\", \"UnitPrice\", \"Month\"], outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[assembler])\n",
    "df_prepared = pipeline.fit(df_ml).transform(df_ml)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6143ad92-bba5-4d69-9cb4-621e3984ce2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir el modelo de Random Forest\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e128c3ea-8267-42f7-b1d5-ed68ea03bccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 535.5743221872216\nR²: 0.008319863384161352\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones en los datos de prueba\n",
    "test_results = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(test_results)\n",
    "r2 = evaluator_r2.evaluate(test_results)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7304f5-0b7a-4bd5-9f11-69961baffff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Análisis de los Resultados del Random Forest\n",
    "RMSE (Error Cuadrático Medio):\n",
    "El RMSE de 535.57 sigue siendo un error alto, lo que indica que las predicciones están bastante lejos de los valores reales del Revenue. Random Forest tiende a funcionar mejor que la regresión lineal en problemas no lineales, pero en este caso, el modelo tampoco parece estar capturando correctamente las relaciones en los datos.\n",
    "\n",
    "R² (Coeficiente de Determinación):\n",
    "El R² de 0.0083 sugiere que el modelo no está logrando explicar la variabilidad del Revenue. Un valor tan bajo de R² significa que casi no hay correlación entre las predicciones y los valores reales del Revenue.\n",
    "\n",
    "¿Qué indica esto?\n",
    "Posibles causas:\n",
    "Sobreajuste (overfitting): Si has entrenado el modelo en un dataset muy pequeño o con variables que no tienen mucha correlación con el target (Revenue), el modelo puede haber memorizado los datos de entrenamiento pero no generaliza bien en el dataset de prueba.\n",
    "Datos insuficientes: Random Forest puede no funcionar bien si los datos que le has dado no tienen suficientes patrones claros, o si las variables que estás utilizando no están bien correlacionadas con el objetivo.\n",
    "Relaciones no capturadas: Puede que las variables Quantity, UnitPrice y Month no sean suficientes para predecir el Revenue de manera precisa. Podría faltar alguna característica más importante.\n",
    "Próximos pasos sugeridos:\n",
    "Validación cruzada: Utiliza técnicas de validación cruzada para obtener una mejor evaluación del rendimiento del modelo.\n",
    "\n",
    "Más características: Considera incluir más variables o características en tu modelo, como podría ser el comportamiento del cliente, el historial de compras, la segmentación geográfica, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e86c26-546e-40cb-98a8-ba7c348d3b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 1: Implementar Validación Cruzada\n",
    "Primero, abordemos la validación cruzada en el contexto de un Random Forest. Este enfoque divide el conjunto de datos en múltiples subconjuntos o \"folds\". Entrenamos el modelo en algunos subconjuntos y lo probamos en otros para obtener una evaluación más robusta de su rendimiento.\n",
    "\n",
    "En PySpark, podemos usar la clase CrossValidator para llevar a cabo este proceso. Aquí te dejo un ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eed2c5b-e90a-4ad4-bf14-053d813203d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "¿Qué es la validación cruzada?\n",
    "La validación cruzada es una técnica que se utiliza para evaluar la eficacia de un modelo de Machine Learning. En vez de entrenar y evaluar el modelo una sola vez, se realizan múltiples particiones en los datos. El objetivo es evitar que los resultados obtenidos dependan de una única división del conjunto de datos (entrenamiento/prueba), lo que podría generar sobreajuste (overfitting) o subajuste (underfitting).\n",
    "\n",
    "Explicación práctica:\n",
    "División en \"folds\": En lugar de entrenar y probar el modelo una vez, dividimos los datos en, por ejemplo, 5 grupos llamados folds.\n",
    "Entrenamiento y validación repetida: Usamos 4 de esos 5 folds para entrenar el modelo y el fold restante para validar. Este proceso se repite cambiando el fold de validación en cada ciclo.\n",
    "Promediamos los resultados: Los errores obtenidos en cada iteración se promedian para dar una estimación más robusta del rendimiento del modelo.\n",
    "La idea detrás de la validación cruzada es obtener un modelo que generalice mejor y que no dependa de una única partición de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f946aba-dc06-4890-a5ed-64eb02a05067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 535.3894753895572\nR²: 0.00900427731731046\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Configurar el grid de parámetros (si quieres hacer un tuning de hiperparámetros)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Configurar el evaluador\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Configurar el CrossValidator\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # 5-fold cross-validation\n",
    "\n",
    "# Entrenar con validación cruzada\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Evaluar en los datos de prueba\n",
    "test_results = cvModel.transform(test_data)\n",
    "rmse = evaluator.evaluate(test_results)\n",
    "r2 = evaluator.evaluate(test_results, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7adb14c-1870-47e8-a230-02fd2693c777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 2: Añadir Más Características (Features)\n",
    "Ya hemos trabajado con Quantity, UnitPrice, y Month. Sin embargo, podemos explorar más variables que puedan capturar mejor la variabilidad del Revenue.\n",
    "\n",
    "Algunas nuevas características a considerar podrían ser:\n",
    "\n",
    "Country: Para ver si el país influye en las ventas.\n",
    "CustomerID: Identificar si algunos clientes generan más ventas que otros.\n",
    "Day of Week o Day of Month: Podría ser relevante ver qué días del mes o de la semana son más productivos.\n",
    "Historias de Ventas: Información del historial del cliente o del producto, como el número de compras anteriores, podría ser útil.\n",
    "Aquí te dejo cómo podrías incluir nuevas columnas y continuar con el proceso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dadb876-85a1-450b-be07-2cbe39f6125d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Explicación del código propuesto:\n",
    "Este código agrega nuevas características a tu conjunto de datos para mejorar el modelo de predicción de ingresos (Revenue). Aquí está el desglose:\n",
    "\n",
    "Nuevas características añadidas:\n",
    "\n",
    "DayOfMonth: El día del mes en que se realizó la compra (del 1 al 31).\n",
    "DayOfWeek: El día de la semana en que se realizó la compra (del 1 al 7).\n",
    "CountryIndex: Un índice numérico para cada país (por ahora convertido directamente, aunque esto podría requerir un tratamiento especial).\n",
    "VectorAssembler: Se actualiza con las nuevas columnas (DayOfMonth, DayOfWeek, CountryIndex), además de las ya existentes (Quantity, UnitPrice, Month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2b0456-41ed-46e2-b912-bf196b8c2cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import dayofmonth, dayofweek\n",
    "\n",
    "# Añadir características adicionales\n",
    "df_ml = df_ml.withColumn(\"DayOfMonth\", dayofmonth(col(\"InvoiceDate\")))\n",
    "df_ml = df_ml.withColumn(\"DayOfWeek\", dayofweek(col(\"InvoiceDate\")))\n",
    "df_ml = df_ml.withColumn(\"CountryIndex\", col(\"Country\").cast(\"int\"))  # Si se puede codificar Country\n",
    "\n",
    "# Actualizamos el assembler con las nuevas columnas\n",
    "assembler = VectorAssembler(inputCols=[\"Quantity\", \"UnitPrice\", \"Month\", \"DayOfMonth\", \"DayOfWeek\", \"CountryIndex\"], outputCol=\"features\")\n",
    "\n",
    "# Continuamos con el pipeline como antes\n",
    "pipeline = Pipeline(stages=[assembler])\n",
    "df_prepared = pipeline.fit(df_ml).transform(df_ml)\n",
    "\n",
    "# Podemos usar el mismo código para entrenar y evaluar el modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3ad27c-9c4f-4596-8467-178a35ad6ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. One-Hot Encoding (OHE)\n",
    "El OHE es una técnica que se utiliza para convertir variables categóricas en un formato que los modelos de Machine Learning puedan entender. Cada categoría se representa como una columna binaria (0 o 1), lo que permite que el modelo procese variables categóricas sin introducir un orden implícito. En este caso, aplicaremos OHE a la columna Country para que el modelo pueda aprovechar esta información de manera más efectiva.\n",
    "\n",
    "2. Implementación de OHE\n",
    "El código que agregué previamente te proporciona una estructura básica. Ahora, aquí te dejo un código que incluye OHE para la columna Country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9434454-af7e-4931-9361-95be7b9af165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n|            features|             label|\n+--------------------+------------------+\n|(42,[0,1,2,3,4,5]...|15.299999999999999|\n|(42,[0,1,2,3,4,5]...|             20.34|\n|(42,[0,1,2,3,4,5]...|              22.0|\n|(42,[0,1,2,3,4,5]...|             20.34|\n|(42,[0,1,2,3,4,5]...|             20.34|\n|(42,[0,1,2,3,4,5]...|              15.3|\n|(42,[0,1,2,3,4,5]...|              25.5|\n|(42,[0,1,2,3,4,5]...|11.100000000000001|\n|(42,[0,1,2,3,4,5]...|11.100000000000001|\n|(42,[0,1,2,3,4,5]...|             54.08|\n+--------------------+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Verificar si la columna 'CountryIndex' ya existe y eliminarla\n",
    "if 'CountryIndex' in df_ml.columns:\n",
    "    df_ml = df_ml.drop('CountryIndex')\n",
    "\n",
    "# Convertir 'Country' a índices numéricos\n",
    "indexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\")\n",
    "\n",
    "# Aplicar One-Hot Encoding a 'CountryIndex'\n",
    "encoder = OneHotEncoder(inputCol=\"CountryIndex\", outputCol=\"CountryVec\")\n",
    "\n",
    "# Actualizar assembler con todas las columnas necesarias\n",
    "assembler = VectorAssembler(inputCols=[\"Quantity\", \"UnitPrice\", \"Month\", \"DayOfMonth\", \"DayOfWeek\", \"CountryVec\"], outputCol=\"features\")\n",
    "\n",
    "# Crear el pipeline con indexer, encoder y assembler\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler])\n",
    "\n",
    "# Transformar los datos\n",
    "df_prepared = pipeline.fit(df_ml).transform(df_ml)\n",
    "\n",
    "# Mostrar las columnas resultantes\n",
    "df_prepared.select(\"features\", \"label\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61483151-2404-4c57-9a70-fa2490382a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Explicación del OHE\n",
    "One-Hot Encoding (OHE) convierte una variable categórica en múltiples columnas binarias. Por ejemplo, si tienes 3 países, United Kingdom, France, y Germany, OHE crea 3 columnas, cada una con 0 o 1, indicando si el valor pertenece a ese país.\n",
    "\n",
    "Ventaja: Evita que el modelo interprete incorrectamente una relación numérica entre las categorías (por ejemplo, si UK=1, France=2, Germany=3, el modelo podría suponer que France > UK, lo cual no tiene sentido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c76635-cd50-4f1a-8d77-9cfce245a881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 535.6690229779704\nR Squared (R²): 0.0079691326186907\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1. Realizar las predicciones en el conjunto de test\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 2. Crear un evaluador para RMSE\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# 3. Crear un evaluador para R²\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Calcular el R²\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R Squared (R²): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fc22b6-7b2a-418a-8564-af8d4c2bbfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "random_forest_regression",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
